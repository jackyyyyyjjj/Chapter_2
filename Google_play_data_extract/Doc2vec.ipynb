{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-Class Text Classification for make-up products using Doc2Vec \n",
    "\n",
    "Multi-Class Text Classification to make-up products based on their description and categories by using Doc2Vec vectors.\n",
    "\n",
    "Doc2vec is a method of vector representation of entire documents, not individual words. By document, you can mean a single sentence, paragraph, or an entire book. Doc2Vec architecture has two algorithms. One of the them is called Distributed Bag of Words (DBOW). The second algorithm is “distributed memory” (DM).\n",
    "\n",
    "In this project, we used Doc2Vec to get the document vectors and we used as input to the classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importing packages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import re\n",
    "from time import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT A.id,A.publisher_url,A.publisher_id,A.publisher_email,A.publisher_name,                     B.id,B.publisher_url,B.publisher_id,B.publisher_email,B.publisher_name                      from AppMonstaPlayStoreDetails20150817 A JOIN AppMonstaPlayStoreDetails20160815 B ON A.id = B.id': no such table: AppMonstaPlayStoreDetails20160815",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py:2020\u001B[0m, in \u001B[0;36mSQLiteDatabase.execute\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2019\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2020\u001B[0m     cur\u001B[38;5;241m.\u001B[39mexecute(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2021\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cur\n",
      "\u001B[1;31mOperationalError\u001B[0m: no such table: AppMonstaPlayStoreDetails20160815",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mDatabaseError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sqlite3\u001B[38;5;241m.\u001B[39mconnect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappmonsta_app.sqlite\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m con:\n\u001B[0;32m      2\u001B[0m     c \u001B[38;5;241m=\u001B[39m con\u001B[38;5;241m.\u001B[39mcursor()\n\u001B[1;32m----> 3\u001B[0m     data\u001B[38;5;241m=\u001B[39m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSELECT A.id,A.publisher_url,A.publisher_id,A.publisher_email,A.publisher_name,\u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;43m                     B.id,B.publisher_url,B.publisher_id,B.publisher_email,B.publisher_name \u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;43m                     from AppMonstaPlayStoreDetails20150817 A JOIN AppMonstaPlayStoreDetails20160815 B ON A.id = B.id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mcon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcon\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py:566\u001B[0m, in \u001B[0;36mread_sql\u001B[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001B[0m\n\u001B[0;32m    563\u001B[0m pandas_sql \u001B[38;5;241m=\u001B[39m pandasSQL_builder(con)\n\u001B[0;32m    565\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pandas_sql, SQLiteDatabase):\n\u001B[1;32m--> 566\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpandas_sql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_query\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43msql\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    569\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoerce_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoerce_float\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    571\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    572\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    573\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    576\u001B[0m     _is_table_name \u001B[38;5;241m=\u001B[39m pandas_sql\u001B[38;5;241m.\u001B[39mhas_table(sql)\n",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py:2080\u001B[0m, in \u001B[0;36mSQLiteDatabase.read_query\u001B[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001B[0m\n\u001B[0;32m   2068\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_query\u001B[39m(\n\u001B[0;32m   2069\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2070\u001B[0m     sql,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2076\u001B[0m     dtype: DtypeArg \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2077\u001B[0m ):\n\u001B[0;32m   2079\u001B[0m     args \u001B[38;5;241m=\u001B[39m _convert_params(sql, params)\n\u001B[1;32m-> 2080\u001B[0m     cursor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2081\u001B[0m     columns \u001B[38;5;241m=\u001B[39m [col_desc[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m col_desc \u001B[38;5;129;01min\u001B[39;00m cursor\u001B[38;5;241m.\u001B[39mdescription]\n\u001B[0;32m   2083\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py:2032\u001B[0m, in \u001B[0;36mSQLiteDatabase.execute\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2029\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ex \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minner_exc\u001B[39;00m\n\u001B[0;32m   2031\u001B[0m ex \u001B[38;5;241m=\u001B[39m DatabaseError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecution failed on sql \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2032\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m ex \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[1;31mDatabaseError\u001B[0m: Execution failed on sql 'SELECT A.id,A.publisher_url,A.publisher_id,A.publisher_email,A.publisher_name,                     B.id,B.publisher_url,B.publisher_id,B.publisher_email,B.publisher_name                      from AppMonstaPlayStoreDetails20150817 A JOIN AppMonstaPlayStoreDetails20160815 B ON A.id = B.id': no such table: AppMonstaPlayStoreDetails20160815"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect('appmonsta_app.sqlite') as con:\n",
    "    c = con.cursor()\n",
    "    data=pd.read_sql('SELECT A.id,A.publisher_url,A.publisher_id,A.publisher_email,A.publisher_name,\\\n",
    "                     B.id,B.publisher_url,B.publisher_id,B.publisher_email,B.publisher_name \\\n",
    "                     from AppMonstaPlayStoreDetails20150817 A JOIN AppMonstaPlayStoreDetails20160815 B ON A.id = B.id',\\\n",
    "                     con=con)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "list = pd.read_excel('sales2.xlsx')\n",
    "list.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "SHIF_list = [['a'],['str']]\n",
    "a = {\n",
    "    'SHIFTDATE': SHIF_list\n",
    "}\n",
    "# 点击下一页\n",
    "\n",
    "offline_a = pd.DataFrame(a)  # 忘了什么意思，但有dataframe应该是转成Excel那样的表格样式吧\n",
    "offline_a.to_excel('随便叫什么.xlsx', index=False)  # 保存到Excel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "offline_a.to_excel('随便叫什么.xlsx', index=False)  # 保存到Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94257\n"
     ]
    }
   ],
   "source": [
    "print(df['description'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have 94 257 words in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Text data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_type    0\n",
       "description     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#changing data type\n",
    "df['description'] = df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#grouping data to smaller number of categories:\n",
    "df.loc[df['product_type'].isin(['lipstick','lip_liner']),'product_type'] = 'lipstic'\n",
    "df.loc[df['product_type'].isin(['blush','bronzer']),'product_type'] = 'contour'\n",
    "df.loc[df['product_type'].isin(['eyeliner','eyeshadow','mascara','eyebrow']),'product_type'] = 'eye_makeup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eye_makeup     367\n",
       "lipstic        176\n",
       "foundation     159\n",
       "contour        144\n",
       "nail_polish     60\n",
       "Name: product_type, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.product_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next step we remove non-alphabetic characters, the stopwords and lemmatizing for each line of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cleanText(words):\n",
    "    \"\"\"The function to clean text\"\"\"\n",
    "    words = re.sub(\"[^a-zA-Z]\",\" \",words)\n",
    "    text = words.lower().split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "df['description'] = df['description'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"The function to removing stopwords\"\"\"\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def word_lem(text):\n",
    "    \"\"\"The function to apply lemmatizing\"\"\"\n",
    "    lem_text = [lem.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['description'] = df['description'].apply(remove_stopwords)\n",
    "df['description'] = df['description'].apply(word_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lipstic</td>\n",
       "      <td>lippie pencil long wearing high intensity lip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lipstic</td>\n",
       "      <td>blotted lip sheer matte lipstick creates perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lipstic</td>\n",
       "      <td>lippie stix formula contains vitamin e mango a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>foundation</td>\n",
       "      <td>developed selfie age buildable full coverage n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lipstic</td>\n",
       "      <td>product free lead heavy metal parabens phthala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_type                                        description\n",
       "0      lipstic  lippie pencil long wearing high intensity lip ...\n",
       "1      lipstic  blotted lip sheer matte lipstick creates perfe...\n",
       "2      lipstic  lippie stix formula contains vitamin e mango a...\n",
       "3   foundation  developed selfie age buildable full coverage n...\n",
       "4      lipstic  product free lead heavy metal parabens phthala..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['description'] = df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60892\n"
     ]
    }
   ],
   "source": [
    "print(df['description'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After text cleaning and removing stop words, we have only 60 892 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#save clean data\n",
    "df.to_csv('C:\\Python Scripts\\API_products\\products_clean.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we split the text into training and testing sets. Then we create a **TaggedDocument** object because this is what Doc2Vec wants as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_tag = train.apply(lambda x: TaggedDocument(words=word_tokenize(x['description']), tags=[x.product_type]), axis=1)\n",
    "\n",
    "test_tag = test.apply(lambda x: TaggedDocument(words=word_tokenize(x['description']), tags=[x.product_type]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['pressed', 'foundation', 'marienatie', 'providing', 'silky', 'flawless', 'finish', 'provides', 'great', 'coverage', 'protects', 'skin', 'spf', 'titanium', 'dioxide', 'act', 'absorbent', 'oil', 'jojoba', 'oil', 'help', 'cleanse', 'moisturize', 'skin'], tags=['foundation'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['let', 'eye', 'naturally', 'pop', 'b', 'smudged', 'subtle', 'eye', 'color', 'add', 'tint', 'color', 'base', 'lash', 'organic', 'cream', 'eye', 'color', 'b', 'smudged', 'eliminates', 'inevitable', 'uneven', 'line', 'traditional', 'eyeliner', 'require', 'expert', 'blending', 'technique', 'messy', 'powder', 'based', 'shadow', 'simply', 'smudge', 'along', 'lash', 'line', 'color', 'stay', 'place', 'long', 'lasting', 'look'], tags=['eye_makeup'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tag[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Getting the feature vector from doc2vec model\n",
    "\n",
    "In this step we initialize the gensim doc2vec model. Doc2Vec architecture is also similar to word2vec and has two algorithms like word2vec and they are the corresponding algorithms for those two algorithms. One of the them is called Distributed Bag of Words (DBOW) which is similar to “Skip-gram” (SG) model in word2vec except that additional paragraph id vector is added. The second algorithm is “distributed memory” (DM) which is similar to “Continuous bag of words” CBOW in word vector.\n",
    "\n",
    "#### Distributed Bag of Words (DBOW)\n",
    "\n",
    "First, we instantiate a doc2vec model — Distributed Bag of Words (DBOW). We set the following parameters: \n",
    "\n",
    "- dm=0 , distributed bag of words (DBOW) is used;\n",
    "- vector_size = 100, word embeddings will have shape of;\n",
    "- window = 2, model will try to predict every second word;\n",
    "- sample=0 , the threshold for configuring which higher-frequency words are randomly down sampled;\n",
    "- min_count=2, ignores all words with total frequency lower than this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "doc_model = Doc2Vec(dm=0, vector_size=100, min_count=2, window=2, sample = 0)\n",
    "               \n",
    "doc_model.build_vocab(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43497"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_model.corpus_total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training for 30 epochs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "%time doc_model.train(train_tag, total_examples=doc_model.corpus_count, epochs=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('beneficial', 0.3377334177494049),\n",
       " ('entire', 0.30722713470458984),\n",
       " ('smoother', 0.2944685220718384),\n",
       " ('fix', 0.2937236726284027),\n",
       " ('intensify', 0.2861747443675995),\n",
       " ('b', 0.2843270003795624),\n",
       " ('page', 0.2822366952896118),\n",
       " ('withstand', 0.279226154088974),\n",
       " ('terracotta', 0.2777043879032135),\n",
       " ('isopropylparaben', 0.2721264958381653)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_model.most_similar('blush')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "doc_model.save('model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Building the Final Vector Feature for the Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train, X_train = vector_for_learning(doc_model, train_tag)\n",
    "y_test, X_test = vector_for_learning(doc_model, test_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training the Classifier\n",
    "\n",
    "We choose Logistic Regression Classifier and Linear Support Vector Machine.\n",
    "\n",
    "***Logistic Regression with DBOW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(n_jobs=1, C=5)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9154411764705882\n",
      "Testing F1 score: 0.9149097862036379\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     contour       0.84      0.90      0.87        40\n",
      "  eye_makeup       0.90      0.98      0.94       115\n",
      "  foundation       0.98      0.87      0.92        53\n",
      "     lipstic       0.93      0.82      0.87        51\n",
      " nail_polish       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.92       272\n",
      "   macro avg       0.93      0.90      0.91       272\n",
      "weighted avg       0.92      0.92      0.91       272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytest = np.array(y_test)\n",
    "print(classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***Linear Support Vector Machine with DBOW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9154411764705882\n",
      "Testing F1 score: 0.9154729253555255\n"
     ]
    }
   ],
   "source": [
    "preds = svm.predict(X_test)\n",
    "print('Testing accuracy %s' % accuracy_score(preds, y_test))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, preds, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     contour       0.82      0.90      0.86        40\n",
      "  eye_makeup       0.91      0.97      0.94       115\n",
      "  foundation       1.00      0.87      0.93        53\n",
      "     lipstic       0.91      0.84      0.88        51\n",
      " nail_polish       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.92       272\n",
      "   macro avg       0.93      0.90      0.91       272\n",
      "weighted avg       0.92      0.92      0.92       272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Distributed Memory (DM)\n",
    "\n",
    "Now, we instantiate a Distributed Memory (DM) with a vector size with 100 words and iterating over the training corpus 30 times.\n",
    "\n",
    "Distributed Memory (DM) works like a memory that remembers what is missing from the current context or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "dm_model = Doc2Vec(dm=1, vector_size=100, min_count=2, window=2, sample = 0, negative=5, alpha=0.025, min_alpha=0.001)\n",
    "dm_model.build_vocab(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43497"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 19s\n"
     ]
    }
   ],
   "source": [
    "%time dm_model.train(train_tag, total_examples=dm_model.corpus_count, epochs=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dm_model.save('dm_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Extract training vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train_dm, X_train_dm = vector_for_learning(dm_model, train_tag)\n",
    "y_test_dm, X_test_dm = vector_for_learning(dm_model, test_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training and testing Classifiers:\n",
    "\n",
    "***Logistic Regression with DM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9007352941176471\n",
      "Testing F1 score: 0.9005217056526574\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(n_jobs=1, C=5)\n",
    "log_reg.fit(X_train_dm, y_train_dm)\n",
    "pred = log_reg.predict(X_test_dm)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test_dm, pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test_dm, pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     contour       0.78      0.90      0.84        40\n",
      "  eye_makeup       0.93      0.97      0.95       115\n",
      "  foundation       0.87      0.87      0.87        53\n",
      "     lipstic       0.95      0.78      0.86        51\n",
      " nail_polish       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.90       272\n",
      "   macro avg       0.91      0.89      0.89       272\n",
      "weighted avg       0.90      0.90      0.90       272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytest = np.array(y_test_dm)\n",
    "print(classification_report(ytest, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***Linear Support Vector Machine with DM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train_dm, y_train_dm)\n",
    "pred_y = svm.predict(X_test_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9007352941176471\n",
      "Testing F1 score: 0.9004546938043879\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy %s' % accuracy_score(pred_y, y_test_dm))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test_dm, pred_y, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     contour       0.80      0.88      0.83        40\n",
      "  eye_makeup       0.95      0.97      0.96       115\n",
      "  foundation       0.84      0.87      0.85        53\n",
      "     lipstic       0.93      0.78      0.85        51\n",
      " nail_polish       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.90       272\n",
      "   macro avg       0.90      0.88      0.89       272\n",
      "weighted avg       0.90      0.90      0.90       272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From received results we can see the obtained accuracy of DBOW model is better than DM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For this project, we used training set to train doc2vec classifier for our make-up products description. We have trained doc2vec ( DBOW and DM) model and decided to use Logistic Regression and Linear Support Vector Machine models, which received the best results in previous testing model (combination Bag of Words with TF-IDF). We were able to achieve 92% accuracy for SVM and Logistic Regression with DBOW method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}